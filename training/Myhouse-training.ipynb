{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import mxnet as mx\n",
    "import numpy as np\n",
    "from mxnet import nd, autograd, gluon\n",
    "import csv\n",
    "import pandas as pd\n",
    "\n",
    "ctx = mx.cpu()\n",
    "\n",
    "# Define some variables\n",
    "\n",
    "# How many samples to process at once when training\n",
    "batch_size = 64\n",
    "\n",
    "# How many classes of gestures you want to have (has to correlate with your data, including \"no-gesture\" class)\n",
    "num_outputs = 7\n",
    "\n",
    "# How many features in each data sample (3 for acceleration, 3 for gravity for a total of 6)\n",
    "num_features_in_sample = 6\n",
    "\n",
    "# How many readings qualify as one complete gesture\n",
    "num_datapoints_in_gesture = 40\n",
    "\n",
    "# How many total readings are in a single gesture\n",
    "num_readings_in_gesture = num_features_in_sample * num_datapoints_in_gesture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load the training data CSV file\n",
    "training_data_reader = pd.read_csv('training-data-complete.csv',parse_dates=['time'])\n",
    "training_data_reader.set_index('time')\n",
    "\n",
    "# Load the testing data CSV file\n",
    "testing_data_reader = pd.read_csv('testing-data.csv',parse_dates=['time'])\n",
    "testing_data_reader.set_index('time')\n",
    "\n",
    "# Do some data massaging to get it to format that the network needs\n",
    "training_data_matrix = training_data_reader.as_matrix()\n",
    "testing_data_matrix = testing_data_reader.as_matrix()\n",
    "\n",
    "training_data = np.asarray(training_data_matrix[:,0:num_features_in_sample], dtype=np.float64)\n",
    "testing_data = np.asarray(testing_data_matrix[:,0:num_features_in_sample], dtype=np.float64)\n",
    "\n",
    "training_data = training_data.reshape(-1, num_datapoints_in_gesture, num_features_in_sample)\n",
    "testing_data = testing_data.reshape(-1, num_datapoints_in_gesture, num_features_in_sample)\n",
    "\n",
    "training_labels = training_data_matrix[:,num_features_in_sample + 1]\n",
    "testing_labels = testing_data_matrix[:,num_features_in_sample + 1]\n",
    "\n",
    "# To prepare training and testing labels, we cut off the dash and the digits at the end of the label name, so \"fan-01\" becomes \"fan\"\n",
    "training_labels = [x[0:-3] for x in training_labels]\n",
    "testing_labels = [x[0:-3] for x in testing_labels]\n",
    "\n",
    "class_dict = {\n",
    "    'tv-poke': 1, \n",
    "    'fan-one-circle': 5, \n",
    "    'fan-two-circles':6, \n",
    "    'shutter-right-left':2, \n",
    "    'no-gesture':0, \n",
    "    'letter-m':3, \n",
    "    'letter-y': 4\n",
    "}\n",
    "\n",
    "# we then turn each label into a corresponding number\n",
    "training_numerical_labels = np.asarray([class_dict[x] for x in training_labels])\n",
    "testing_numerical_labels = np.asarray([class_dict[x] for x in testing_labels])\n",
    "\n",
    "# now, get a list of all labels\n",
    "training_labels = training_numerical_labels[0:-1:num_datapoints_in_gesture]\n",
    "testing_labels = testing_numerical_labels[0:-1:num_datapoints_in_gesture]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the datasets from the training data and labels, do the same for testing data and labels\n",
    "training_dataset_array = mx.gluon.data.ArrayDataset(mx.nd.array(training_data), mx.nd.array(training_labels))\n",
    "testing_dataset_array = mx.gluon.data.ArrayDataset(mx.nd.array(testing_data), mx.nd.array(testing_labels))\n",
    "\n",
    "# Load the data, shuffling the training data and using the batch size mentioned earlier\n",
    "training_data = mx.gluon.data.DataLoader(training_dataset_array, batch_size=batch_size, shuffle=True)\n",
    "testing_data = mx.gluon.data.DataLoader(testing_dataset_array, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Construct a multi layer perceptron, with some hidden layers\n",
    "# define number of neurons in each hidden layer\n",
    "num_hidden = 128\n",
    "\n",
    "net = gluon.nn.Sequential()\n",
    "with net.name_scope():\n",
    "    ###########################\n",
    "    # Adding first hidden layer\n",
    "    ###########################\n",
    "    net.add(gluon.nn.Dense(num_hidden, activation=\"relu\"))\n",
    "    ###########################\n",
    "    # Adding dropout with rate .5 to the first hidden layer\n",
    "    ###########################\n",
    "    net.add(gluon.nn.Dropout(.5))\n",
    "\n",
    "    ###########################\n",
    "    # Adding first hidden layer\n",
    "    ###########################\n",
    "    net.add(gluon.nn.Dense(num_hidden, activation=\"relu\"))\n",
    "    ###########################\n",
    "    # Adding dropout with rate .5 to the second hidden layer\n",
    "    ###########################\n",
    "    net.add(gluon.nn.Dropout(.5))\n",
    "\n",
    "    ###########################\n",
    "    # Adding the output layer\n",
    "    ###########################\n",
    "    net.add(gluon.nn.Dense(num_outputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initialize the neural network\n",
    "net.collect_params().initialize(mx.init.Xavier(magnitude=2.24), ctx=ctx, force_reinit=True)\n",
    "softmax_cross_entropy = gluon.loss.SoftmaxCrossEntropyLoss()\n",
    "# Define the trainer\n",
    "trainer = gluon.Trainer(net.collect_params(), 'sgd', {'learning_rate': .05})\n",
    "\n",
    "# Create a function for accuracy evaluation\n",
    "def evaluate_accuracy(data_iterator, net):\n",
    "    acc = mx.metric.Accuracy()\n",
    "    for i, (data, label) in enumerate(data_iterator):\n",
    "        data = data.as_in_context(ctx).reshape((-1, num_readings_in_gesture))\n",
    "        label = label.as_in_context(ctx)\n",
    "        output = net(data)\n",
    "        predictions = nd.argmax(output, axis=1)\n",
    "        acc.update(preds=predictions, labels=label)\n",
    "    return acc.get()[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0. Loss: 2.703669393776569, Train_acc 0.7387755102040816, Test_acc 0.6571428571428571\n",
      "Epoch 1. Loss: 2.5976105280859283, Train_acc 0.8408163265306122, Test_acc 0.8142857142857143\n",
      "Epoch 2. Loss: 2.478828572953365, Train_acc 0.8816326530612245, Test_acc 0.8571428571428571\n",
      "Epoch 3. Loss: 2.356115037171035, Train_acc 0.9306122448979591, Test_acc 0.9\n",
      "Epoch 4. Loss: 2.230359458807729, Train_acc 0.963265306122449, Test_acc 0.9142857142857143\n",
      "Epoch 5. Loss: 2.1088775781135576, Train_acc 0.9693877551020408, Test_acc 0.9142857142857143\n",
      "Epoch 6. Loss: 1.990788033524717, Train_acc 0.9734693877551021, Test_acc 0.8857142857142857\n",
      "Epoch 7. Loss: 1.878448196202987, Train_acc 0.9714285714285714, Test_acc 0.9285714285714286\n",
      "Epoch 8. Loss: 1.771624543216809, Train_acc 0.9775510204081632, Test_acc 0.9285714285714286\n",
      "Epoch 9. Loss: 1.6655612556574448, Train_acc 0.9775510204081632, Test_acc 0.9\n",
      "Epoch 10. Loss: 1.566727924601048, Train_acc 0.9775510204081632, Test_acc 0.9285714285714286\n",
      "Epoch 11. Loss: 1.4766251487508641, Train_acc 0.9775510204081632, Test_acc 0.9285714285714286\n",
      "Epoch 12. Loss: 1.3883550278717627, Train_acc 0.9795918367346939, Test_acc 0.9285714285714286\n",
      "Epoch 13. Loss: 1.30679365274814, Train_acc 0.9734693877551021, Test_acc 0.9142857142857143\n",
      "Epoch 14. Loss: 1.2269754877003303, Train_acc 0.9795918367346939, Test_acc 0.9142857142857143\n",
      "Epoch 15. Loss: 1.1554263408917285, Train_acc 0.9775510204081632, Test_acc 0.9428571428571428\n",
      "Epoch 16. Loss: 1.0898788588145774, Train_acc 0.9775510204081632, Test_acc 0.9428571428571428\n",
      "Epoch 17. Loss: 1.0239926825082692, Train_acc 0.9816326530612245, Test_acc 0.9285714285714286\n",
      "Epoch 18. Loss: 0.9655178299540029, Train_acc 0.9857142857142858, Test_acc 0.9285714285714286\n",
      "Epoch 19. Loss: 0.9055503597000085, Train_acc 0.9857142857142858, Test_acc 0.9285714285714286\n",
      "Epoch 20. Loss: 0.8526558732121927, Train_acc 0.9877551020408163, Test_acc 0.9285714285714286\n",
      "Epoch 21. Loss: 0.8011087346202521, Train_acc 0.9897959183673469, Test_acc 0.9285714285714286\n",
      "Epoch 22. Loss: 0.7554284405625699, Train_acc 0.9857142857142858, Test_acc 0.9428571428571428\n",
      "Epoch 23. Loss: 0.7110906061485877, Train_acc 0.9877551020408163, Test_acc 0.9285714285714286\n",
      "Epoch 24. Loss: 0.6692286041986685, Train_acc 0.9816326530612245, Test_acc 0.9428571428571428\n",
      "Epoch 25. Loss: 0.6293287178710049, Train_acc 0.9877551020408163, Test_acc 0.9285714285714286\n",
      "Epoch 26. Loss: 0.5936996992910643, Train_acc 0.9897959183673469, Test_acc 0.9285714285714286\n",
      "Epoch 27. Loss: 0.55663172512766, Train_acc 0.9897959183673469, Test_acc 0.9285714285714286\n",
      "Epoch 28. Loss: 0.5251988114450891, Train_acc 0.9897959183673469, Test_acc 0.9285714285714286\n",
      "Epoch 29. Loss: 0.49472078854185536, Train_acc 0.9897959183673469, Test_acc 0.9285714285714286\n",
      "Epoch 30. Loss: 0.4676720879158663, Train_acc 0.9897959183673469, Test_acc 0.9285714285714286\n",
      "Epoch 31. Loss: 0.4424187033615535, Train_acc 0.9897959183673469, Test_acc 0.9285714285714286\n",
      "Epoch 32. Loss: 0.4195870719685053, Train_acc 0.9897959183673469, Test_acc 0.9285714285714286\n",
      "Epoch 33. Loss: 0.3973045677655389, Train_acc 0.9918367346938776, Test_acc 0.9285714285714286\n",
      "Epoch 34. Loss: 0.37715860765540854, Train_acc 0.9918367346938776, Test_acc 0.9428571428571428\n",
      "Epoch 35. Loss: 0.357082450688552, Train_acc 0.9897959183673469, Test_acc 0.9428571428571428\n",
      "Epoch 36. Loss: 0.3377445959300786, Train_acc 0.9918367346938776, Test_acc 0.9285714285714286\n",
      "Epoch 37. Loss: 0.3221727052277338, Train_acc 0.9918367346938776, Test_acc 0.9285714285714286\n",
      "Epoch 38. Loss: 0.3059248479623634, Train_acc 0.9918367346938776, Test_acc 0.9285714285714286\n",
      "Epoch 39. Loss: 0.29039342241400623, Train_acc 0.9918367346938776, Test_acc 0.9285714285714286\n",
      "Epoch 40. Loss: 0.27609844336165507, Train_acc 0.9938775510204082, Test_acc 0.9428571428571428\n",
      "Epoch 41. Loss: 0.26394552518886116, Train_acc 0.9938775510204082, Test_acc 0.9285714285714286\n",
      "Epoch 42. Loss: 0.25295615764363294, Train_acc 0.9938775510204082, Test_acc 0.9285714285714286\n",
      "Epoch 43. Loss: 0.24030234856854016, Train_acc 0.9938775510204082, Test_acc 0.9285714285714286\n",
      "Epoch 44. Loss: 0.22888443184138346, Train_acc 0.9959183673469387, Test_acc 0.9285714285714286\n",
      "Epoch 45. Loss: 0.21628272945032007, Train_acc 0.9959183673469387, Test_acc 0.9285714285714286\n",
      "Epoch 46. Loss: 0.20614345956570834, Train_acc 0.9959183673469387, Test_acc 0.9285714285714286\n",
      "Epoch 47. Loss: 0.19608727668308215, Train_acc 0.9979591836734694, Test_acc 0.9285714285714286\n",
      "Epoch 48. Loss: 0.1863503269489409, Train_acc 0.9979591836734694, Test_acc 0.9285714285714286\n",
      "Epoch 49. Loss: 0.17850259629150778, Train_acc 0.9979591836734694, Test_acc 0.9285714285714286\n",
      "Epoch 50. Loss: 0.17357533490122504, Train_acc 0.9979591836734694, Test_acc 0.9285714285714286\n",
      "Epoch 51. Loss: 0.16507835760555378, Train_acc 0.9979591836734694, Test_acc 0.9285714285714286\n",
      "Epoch 52. Loss: 0.15706727187439304, Train_acc 1.0, Test_acc 0.9285714285714286\n",
      "Epoch 53. Loss: 0.15010928193213519, Train_acc 1.0, Test_acc 0.9285714285714286\n",
      "Epoch 54. Loss: 0.14696147854206013, Train_acc 1.0, Test_acc 0.9285714285714286\n",
      "Epoch 55. Loss: 0.1417421059714105, Train_acc 0.9979591836734694, Test_acc 0.9285714285714286\n",
      "Epoch 56. Loss: 0.13657304281336516, Train_acc 0.9979591836734694, Test_acc 0.9285714285714286\n",
      "Epoch 57. Loss: 0.13585862350931965, Train_acc 0.9979591836734694, Test_acc 0.9285714285714286\n",
      "Epoch 58. Loss: 0.1305526144385945, Train_acc 0.9979591836734694, Test_acc 0.9285714285714286\n",
      "Epoch 59. Loss: 0.1256647132518815, Train_acc 0.9979591836734694, Test_acc 0.9285714285714286\n",
      "Epoch 60. Loss: 0.12030641820986977, Train_acc 0.9979591836734694, Test_acc 0.9285714285714286\n",
      "Epoch 61. Loss: 0.11610459954501971, Train_acc 0.9979591836734694, Test_acc 0.9285714285714286\n",
      "Epoch 62. Loss: 0.11489292572948419, Train_acc 1.0, Test_acc 0.9428571428571428\n",
      "Epoch 63. Loss: 0.11028163592333246, Train_acc 0.9979591836734694, Test_acc 0.9285714285714286\n",
      "Epoch 64. Loss: 0.10797415559971091, Train_acc 0.9979591836734694, Test_acc 0.9285714285714286\n",
      "Epoch 65. Loss: 0.1040641694907908, Train_acc 1.0, Test_acc 0.9571428571428572\n",
      "Epoch 66. Loss: 0.10116251256977699, Train_acc 1.0, Test_acc 0.9428571428571428\n",
      "Epoch 67. Loss: 0.09928854312081137, Train_acc 1.0, Test_acc 0.9428571428571428\n",
      "Epoch 68. Loss: 0.09594588853828992, Train_acc 1.0, Test_acc 0.9428571428571428\n",
      "Epoch 69. Loss: 0.09305605927494996, Train_acc 1.0, Test_acc 0.9428571428571428\n",
      "Epoch 70. Loss: 0.0908194628710803, Train_acc 1.0, Test_acc 0.9428571428571428\n",
      "Epoch 71. Loss: 0.087948014516042, Train_acc 1.0, Test_acc 0.9428571428571428\n",
      "Epoch 72. Loss: 0.08622025042007213, Train_acc 1.0, Test_acc 0.9428571428571428\n",
      "Epoch 73. Loss: 0.08329308275939805, Train_acc 0.9979591836734694, Test_acc 0.9285714285714286\n",
      "Epoch 74. Loss: 0.08060983593575471, Train_acc 1.0, Test_acc 0.9428571428571428\n",
      "Epoch 75. Loss: 0.07850316525082883, Train_acc 1.0, Test_acc 0.9428571428571428\n",
      "Epoch 76. Loss: 0.07557580975498422, Train_acc 1.0, Test_acc 0.9428571428571428\n",
      "Epoch 77. Loss: 0.07405092238327123, Train_acc 1.0, Test_acc 0.9285714285714286\n",
      "Epoch 78. Loss: 0.07186996927010254, Train_acc 1.0, Test_acc 0.9428571428571428\n",
      "Epoch 79. Loss: 0.06949705100774532, Train_acc 1.0, Test_acc 0.9428571428571428\n",
      "Epoch 80. Loss: 0.06776039631230495, Train_acc 1.0, Test_acc 0.9285714285714286\n",
      "Epoch 81. Loss: 0.06585707499402742, Train_acc 1.0, Test_acc 0.9285714285714286\n",
      "Epoch 82. Loss: 0.06601244705060556, Train_acc 1.0, Test_acc 0.9428571428571428\n",
      "Epoch 83. Loss: 0.06474219054593675, Train_acc 1.0, Test_acc 0.9428571428571428\n",
      "Epoch 84. Loss: 0.062214740210918426, Train_acc 1.0, Test_acc 0.9428571428571428\n",
      "Epoch 85. Loss: 0.06276996617762959, Train_acc 1.0, Test_acc 0.9428571428571428\n",
      "Epoch 86. Loss: 0.061181341839395574, Train_acc 1.0, Test_acc 0.9428571428571428\n",
      "Epoch 87. Loss: 0.060748236701610454, Train_acc 1.0, Test_acc 0.9428571428571428\n",
      "Epoch 88. Loss: 0.05877465888130652, Train_acc 1.0, Test_acc 0.9428571428571428\n",
      "Epoch 89. Loss: 0.05702738790056185, Train_acc 1.0, Test_acc 0.9285714285714286\n",
      "Epoch 90. Loss: 0.055292361115352406, Train_acc 1.0, Test_acc 0.9428571428571428\n",
      "Epoch 91. Loss: 0.05432364242432524, Train_acc 1.0, Test_acc 0.9428571428571428\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 92. Loss: 0.05183837073639754, Train_acc 1.0, Test_acc 0.9428571428571428\n",
      "Epoch 93. Loss: 0.05159497876067116, Train_acc 1.0, Test_acc 0.9428571428571428\n",
      "Epoch 94. Loss: 0.051156354086510585, Train_acc 1.0, Test_acc 0.9428571428571428\n",
      "Epoch 95. Loss: 0.04987028647319851, Train_acc 1.0, Test_acc 0.9428571428571428\n",
      "Epoch 96. Loss: 0.047527554025583986, Train_acc 1.0, Test_acc 0.9428571428571428\n",
      "Epoch 97. Loss: 0.047483288422397955, Train_acc 1.0, Test_acc 0.9428571428571428\n",
      "Epoch 98. Loss: 0.04624384158835569, Train_acc 1.0, Test_acc 0.9428571428571428\n",
      "Epoch 99. Loss: 0.04655477974745708, Train_acc 1.0, Test_acc 0.9428571428571428\n",
      "Epoch 100. Loss: 0.04513527699145481, Train_acc 1.0, Test_acc 0.9428571428571428\n",
      "Epoch 101. Loss: 0.04390019384034757, Train_acc 1.0, Test_acc 0.9428571428571428\n",
      "Epoch 102. Loss: 0.04470597703953521, Train_acc 1.0, Test_acc 0.9428571428571428\n",
      "Epoch 103. Loss: 0.043947642225778905, Train_acc 1.0, Test_acc 0.9428571428571428\n",
      "Epoch 104. Loss: 0.04257085507470526, Train_acc 1.0, Test_acc 0.9428571428571428\n",
      "Epoch 105. Loss: 0.04209004757508685, Train_acc 1.0, Test_acc 0.9428571428571428\n",
      "Epoch 106. Loss: 0.0413805204679311, Train_acc 1.0, Test_acc 0.9428571428571428\n",
      "Epoch 107. Loss: 0.04031643521136997, Train_acc 1.0, Test_acc 0.9428571428571428\n",
      "Epoch 108. Loss: 0.039545996287865245, Train_acc 1.0, Test_acc 0.9428571428571428\n",
      "Epoch 109. Loss: 0.03831106272616275, Train_acc 1.0, Test_acc 0.9428571428571428\n",
      "Epoch 110. Loss: 0.03768717381643159, Train_acc 1.0, Test_acc 0.9428571428571428\n",
      "Epoch 111. Loss: 0.03729710535924706, Train_acc 1.0, Test_acc 0.9428571428571428\n",
      "Epoch 112. Loss: 0.03601985181065857, Train_acc 1.0, Test_acc 0.9428571428571428\n",
      "Epoch 113. Loss: 0.03604266425630714, Train_acc 1.0, Test_acc 0.9428571428571428\n",
      "Epoch 114. Loss: 0.0350337905326932, Train_acc 1.0, Test_acc 0.9428571428571428\n",
      "Epoch 115. Loss: 0.03402895949928958, Train_acc 1.0, Test_acc 0.9428571428571428\n",
      "Epoch 116. Loss: 0.03265409215971482, Train_acc 1.0, Test_acc 0.9428571428571428\n",
      "Epoch 117. Loss: 0.033015764485153094, Train_acc 1.0, Test_acc 0.9428571428571428\n",
      "Epoch 118. Loss: 0.03325929019222304, Train_acc 1.0, Test_acc 0.9428571428571428\n",
      "Epoch 119. Loss: 0.033630132749157875, Train_acc 1.0, Test_acc 0.9428571428571428\n"
     ]
    }
   ],
   "source": [
    "# Define some training parameters\n",
    "\n",
    "# How many epochs to run the training (try various numbers)\n",
    "epochs = 120\n",
    "\n",
    "smoothing_constant = .01\n",
    "\n",
    "for e in range(epochs):\n",
    "    for i, (data, label) in enumerate(training_data):\n",
    "        data = data.as_in_context(ctx).reshape((-1, num_readings_in_gesture))\n",
    "        # Comment this out to add some noise to the data\n",
    "        data = data + 0.1*nd.mean(nd.abs(data)) * nd.random.normal(shape=data.shape)\n",
    "        label = label.as_in_context(ctx)\n",
    "        with autograd.record():\n",
    "            output = net(data)\n",
    "            loss = softmax_cross_entropy(output, label)\n",
    "            loss.backward()\n",
    "        trainer.step(data.shape[0])\n",
    "\n",
    "        ##########################\n",
    "        #  Keep a moving average of the losses\n",
    "        ##########################\n",
    "        curr_loss = nd.mean(loss).asscalar()\n",
    "        moving_loss = (curr_loss if ((i == 0) and (e == 0))\n",
    "                       else (1 - smoothing_constant) * moving_loss + (smoothing_constant) * curr_loss)\n",
    "\n",
    "    test_accuracy = evaluate_accuracy(testing_data, net)\n",
    "    train_accuracy = evaluate_accuracy(training_data, net)\n",
    "    print(\"Epoch %s. Loss: %s, Train_acc %s, Test_acc %s\" %\n",
    "          (e, moving_loss, train_accuracy, test_accuracy))\n",
    "\n",
    "# After training, we can save the trained parameters to disk\n",
    "net.save_params('ssd_%d-testing.params' % epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The network's guess is: \n",
      "\n",
      "shutter-right-left\n"
     ]
    }
   ],
   "source": [
    "# Load raw data from a known gesture (shutter on / off)\n",
    "single_sample_reader = pd.read_csv('shutter.csv')\n",
    "\n",
    "single_sample_matrix = single_sample_reader.as_matrix()\n",
    "single_sample_data = np.asarray(single_sample_matrix[:,0:6], dtype=np.float64)\n",
    "single_sample_data = single_sample_data.reshape(-1, num_datapoints_in_gesture, num_features_in_sample)\n",
    "single_sample_data = single_sample_data.reshape(-1, num_readings_in_gesture)\n",
    "\n",
    "# pass the data from a single gesture to the trained network to verify the result\n",
    "result_num = int(net(mx.nd.array(single_sample_data)).argmax(axis=1).asscalar())\n",
    "\n",
    "result_dict = { \n",
    "    1:'tv-poke', \n",
    "    5:'fan-one-circle', \n",
    "    6:'fan-two-circles', \n",
    "    2:'shutter-right-left', \n",
    "    0:'no-gesture', \n",
    "    3:'letter-m', \n",
    "    4:'letter-y'\n",
    "    \n",
    "}\n",
    "\n",
    "print(\"The network's guess is: \\n\")\n",
    "\n",
    "print(result_dict[result_num])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
